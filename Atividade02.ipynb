{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atividade02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9u+WXCnaS04IkOgfF6zp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArturHugo/PLN-2022-1/blob/main/Atividade02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade 01 - Prática de Expressões Regulares\n",
        "Grupo:\n",
        "\n",
        "- Artur Hugo cunha Pereira, 180030400\n",
        "- Gabriel da Silva Corvino Nogueira, 180113330 \n"
      ],
      "metadata": {
        "id": "QOHZl0bgq7bx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ygAmwSR-q3ty"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/viniciusrpb/cic0269_natural_language_processing/main/corpus_tweets/twitter-2013train-A.txt', sep='\\t', names=['id', 'sentiment', 'tweet'])\n",
        "df.values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BO3ckKPrXWB",
        "outputId": "d99e060d-5ef3-4eec-8d17-c43516aa11fb"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([264183816548130816, 'positive',\n",
              "       'Gas by my house hit $3.39!!!! I\\\\u2019m going to Chapel Hill on Sat. :)'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df = df['tweet'][0:5]\n",
        "tweet_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ6GNQEysoZw",
        "outputId": "10653182-28da-431c-c56b-31e0f21e8d9c"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Gas by my house hit $3.39!!!! I\\u2019m going t...\n",
              "1    Theo Walcott is still shit\\u002c watch Rafa an...\n",
              "2    its not that I\\u2019m a GSP fan\\u002c i just h...\n",
              "3    Iranian general says Israel\\u2019s Iron Dome c...\n",
              "4    Tehran\\u002c Mon Amour: Obama Tried to Establi...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stemmer = nltk.stem.porter.PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peNleru71Qzl",
        "outputId": "c817e8ca-388d-4351-c961-cc82638fd1ae"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def tokenize(string):\n",
        "  string.replace(':)', 'happy') #  use re\n",
        "  string.replace(':(', 'sad')\n",
        "  res = re.split(r'\\'|@\\w+|\\\\u[0-9a-f]{4}|[\\.\\:]\\s|!|\\$', string.lower())\n",
        "  res = nltk.tokenize.word_tokenize(' '.join(res))\n",
        "  return res\n",
        "\n",
        "def filter_stopwords(word_array, stop_list):\n",
        "  res = [word for word in word_array if word not in stop_list]\n",
        "  return res"
      ],
      "metadata": {
        "id": "fwtJIR0Rwl0_"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "tweet_df = tweet_df.apply(tokenize)\n",
        "tweet_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3-xX02PwNnW",
        "outputId": "489c8f39-c594-4d73-d556-3c8ac592952c"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [gas, by, my, house, hit, 3.39, i, m, going, t...\n",
              "1    [theo, walcott, is, still, shit, watch, rafa, ...\n",
              "2    [its, not, that, i, m, a, gsp, fan, i, just, h...\n",
              "3    [iranian, general, says, israel, s, iron, dome...\n",
              "4    [tehran, mon, amour, obama, tried, to, establi...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "stop_list = [element[0] for element in pd.read_csv('https://raw.githubusercontent.com/ravikiranj/twitter-sentiment-analyzer/master/data/feature_list/stopwords.txt', header=None).values]\n",
        "print(stop_list)\n",
        "tweet_df = tweet_df.apply(lambda tweet: filter_stopwords(tweet, stop_list))\n",
        "tweet_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEsud-pizuM3",
        "outputId": "2236dba2-e6b0-4d9f-8820-5516af13801a"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [gas, house, hit, 3.39, chapel, hill, sat, :, )]\n",
              "1    [theo, walcott, shit, watch, rafa, johnny, dea...\n",
              "2      [gsp, fan, hate, nick, diaz, wait, february, .]\n",
              "3    [iranian, israel, iron, dome, deal, missiles, ...\n",
              "4    [tehran, mon, amour, obama, tried, establish, ...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "tweet_df = tweet_df.apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
        "tweet_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFJLDtSg4ZDt",
        "outputId": "e2dd942f-63ea-485e-cf9e-43b1516512dc"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [ga, hous, hit, 3.39, chapel, hill, sat, :, )]\n",
              "1    [theo, walcott, shit, watch, rafa, johnni, dea...\n",
              "2      [gsp, fan, hate, nick, diaz, wait, februari, .]\n",
              "3    [iranian, israel, iron, dome, deal, missil, (,...\n",
              "4    [tehran, mon, amour, obama, tri, establish, ti...\n",
              "Name: tweet, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_df.values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTevS9sZ-JnM",
        "outputId": "911dcded-61f4-4512-d751-fc1f4e28e814"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ga', 'hous', 'hit', '3.39', 'chapel', 'hill', 'sat', ':', ')']"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    }
  ]
}